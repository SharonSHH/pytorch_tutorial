{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置函数下载 mnist 数据集\n",
    "train_set = mnist.MNIST('./data', train=True, download=True)\n",
    "test_set = mnist.MNIST('./data', train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_data, a_label = train_set[0]\n",
    "a_data = np.array(a_data, dtype='float32')\n",
    "print(a_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.reshape((-1,)) # 拉平\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# 使用 pytorch 自带的 DataLoader 定义一个数据迭代器\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, a_label = next(iter(train_data))\n",
    "# 使用 Sequential 定义 4 层神经网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 400),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(400, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 loss 函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train Loss: 0.501213, Train Acc: 0.834871, Eval Loss: 0.218925, Eval Acc: 0.929391\n",
      "epoch: 1, Train Loss: 0.169393, Train Acc: 0.946862, Eval Loss: 0.129550, Eval Acc: 0.957773\n",
      "epoch: 2, Train Loss: 0.116965, Train Acc: 0.963136, Eval Loss: 0.184991, Eval Acc: 0.937500\n",
      "epoch: 3, Train Loss: 0.094204, Train Acc: 0.970566, Eval Loss: 0.114190, Eval Acc: 0.963509\n",
      "epoch: 4, Train Loss: 0.074499, Train Acc: 0.976712, Eval Loss: 0.114613, Eval Acc: 0.964201\n",
      "epoch: 5, Train Loss: 0.065042, Train Acc: 0.979244, Eval Loss: 0.083570, Eval Acc: 0.973794\n",
      "epoch: 6, Train Loss: 0.053576, Train Acc: 0.982776, Eval Loss: 0.245770, Eval Acc: 0.927512\n",
      "epoch: 7, Train Loss: 0.044384, Train Acc: 0.985491, Eval Loss: 0.073522, Eval Acc: 0.977947\n",
      "epoch: 8, Train Loss: 0.042377, Train Acc: 0.986191, Eval Loss: 0.071717, Eval Acc: 0.977255\n",
      "epoch: 9, Train Loss: 0.034203, Train Acc: 0.988739, Eval Loss: 0.103164, Eval Acc: 0.969937\n",
      "epoch: 10, Train Loss: 0.028180, Train Acc: 0.990922, Eval Loss: 0.069788, Eval Acc: 0.979134\n",
      "epoch: 11, Train Loss: 0.024123, Train Acc: 0.991921, Eval Loss: 0.123482, Eval Acc: 0.965289\n",
      "epoch: 12, Train Loss: 0.024506, Train Acc: 0.992504, Eval Loss: 0.084235, Eval Acc: 0.976365\n",
      "epoch: 13, Train Loss: 0.021039, Train Acc: 0.993220, Eval Loss: 0.069343, Eval Acc: 0.981013\n",
      "epoch: 14, Train Loss: 0.018016, Train Acc: 0.994037, Eval Loss: 0.080039, Eval Acc: 0.979233\n",
      "epoch: 15, Train Loss: 0.013348, Train Acc: 0.995436, Eval Loss: 0.077031, Eval Acc: 0.978738\n",
      "epoch: 16, Train Loss: 0.014548, Train Acc: 0.995319, Eval Loss: 0.082720, Eval Acc: 0.977650\n",
      "epoch: 17, Train Loss: 0.011435, Train Acc: 0.996435, Eval Loss: 0.069147, Eval Acc: 0.982496\n",
      "epoch: 18, Train Loss: 0.008238, Train Acc: 0.997585, Eval Loss: 0.076435, Eval Acc: 0.981309\n",
      "epoch: 19, Train Loss: 0.009287, Train Acc: 0.997251, Eval Loss: 0.071578, Eval Acc: 0.982793\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "for e in range(20):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net.train()\n",
    "    for im, label in train_data:\n",
    "        im = Variable(im)\n",
    "        label = Variable(label)\n",
    "        # 前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 记录误差\n",
    "        train_loss += loss.item()\n",
    "        # 计算分类的准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        train_acc += acc\n",
    "        \n",
    "    losses.append(train_loss / len(train_data))\n",
    "    acces.append(train_acc / len(train_data))\n",
    "    # 在测试集上检验效果\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    net.eval() # 将模型改为预测模式\n",
    "    for im, label in test_data:\n",
    "        im = Variable(im)\n",
    "        label = Variable(label)\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 记录误差\n",
    "        eval_loss += loss.item()\n",
    "        # 记录准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        eval_acc += acc\n",
    "        \n",
    "    eval_losses.append(eval_loss / len(test_data))\n",
    "    eval_acces.append(eval_acc / len(test_data))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}, Eval Loss: {:.6f}, Eval Acc: {:.6f}'\n",
    "          .format(e, train_loss / len(train_data), train_acc / len(train_data), \n",
    "                     eval_loss / len(test_data), eval_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
